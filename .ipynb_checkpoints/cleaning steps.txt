The first step I took in cleaning my dataset was identifying which columns were relevant to my project. I narrowed down the variables I am interested in to year, month, region, attack type, target type, weapon type, success (whether the attack was successful), number killed, number wounded, whether there was property damage, and the extent of property damage.

Next, I dealt with missing values. The dataset I used encodes missing values in two ways: blank fields, and specific codes indicating that the value for a column is unknown. I used the na_values parameter in the Pandas read_csv function to read the unknown value codes as null values. 

After that, I filtered the dataframe for rows with null values. I had to be careful in which columns I used to filter. In this dataset, attack type, target type, and weapon type are represented by several columns because some events fall under multiple types of attack, target, and weapon. However, most rows have only one type for each of these sets of columns, so I only filtered by the first column (if the first column is null, the second and third will be too). I have not yet decided how I will handle the multiple columns when it comes to analysis. One option would be to only look at the first column, which would make no difference for the large majority of rows. The order of the types is determined by set hierarchies, and I will need to decide whether the hierarchies capture which types are most relevant. Another option would be to pivot these columns so that each possible type value has its own column as a binary variable. This would create a fairly larger number of columns (there are 8 possible attack types, 21 possible target types, and 12 possible weapon types). In addition, extent of property damage is null when property damage is false, so I only excluded rows where property damage is True, but extent of property damage is null.

This brought the number of rows down to 82,509, which is slightly less than half than the original 170,350. I believe this cleaning process may be too stringent. In particular, filtering by extent of property damage by itself removed around 40,000 rows, which is  a decision I will need to revisit. 

For the number killed and number wounded variables, I examined the data for outliers. Both variables are right-skewed, with maximum values of 1383 and 7366 respectively, compared to medians of 0 and 75th percentiles of 2 for both variables. The 99th percentiles are 21 and 38, respectively. I suspect that I will need to exclude the largest values if I want to do a meaningful regression analysis for these variables, but I am unsure of what to do when there are a large number of rows that are far outside the IQR. For now, I have not excluded any outliers.