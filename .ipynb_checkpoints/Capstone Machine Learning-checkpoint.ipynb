{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Progress summary:**\n",
    "\n",
    "Tried logistic regression and random forests to predict success. Logistic regression predicted all attacks as successful; random forests did predict some failures but had low recall for failures.\n",
    "\n",
    "Also tried logistic regression, random forests, and SVM for predicting whether an attack results in any fatalities. Accuracy scores for all were 70-something percent (in dataset, 45% of attacks result in fatalities). Overfitting does not seem to be a problem right now, so adding more explanatory variables to boost accuracy might help.\n",
    "\n",
    "**to do/questions:**\n",
    "    \n",
    "how to improve recall for unsuccessful attacks (how to give greater weight to recall for unsuccessful attacks when tuning)\n",
    "\n",
    "how to run SVC faster to properly tune it\n",
    "\n",
    "replace success with fatality rate or another metric? improving recall just means trying to pin down the small proportion of attempted attacks that aren't successful. But when most attacks in the dataset are successful, identifying failures doesn't matter that much. In this context, identifying a failure is less important than identifying successes (while if it were the other way around, 90% failure and 10% success, improving recall for success would be very important)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#does cross-validation and computes average score for the 5 folds\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold, random_state = 42).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y.iloc[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y.iloc[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned.csv')\n",
    "df['fatal'] = df['nkill'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 122376 entries, 0 to 122375\n",
      "Data columns (total 25 columns):\n",
      "Unnamed: 0     122376 non-null int64\n",
      "iyear          122376 non-null int64\n",
      "imonth         122376 non-null int64\n",
      "region         122376 non-null int64\n",
      "crit1          122376 non-null int64\n",
      "crit2          122376 non-null int64\n",
      "crit3          122376 non-null int64\n",
      "success        122376 non-null int64\n",
      "suicide        122376 non-null int64\n",
      "attacktype1    122376 non-null float64\n",
      "attacktype2    3533 non-null float64\n",
      "attacktype3    214 non-null float64\n",
      "targtype1      122376 non-null float64\n",
      "targtype2      6685 non-null float64\n",
      "targtype3      703 non-null float64\n",
      "individual     122376 non-null int64\n",
      "weaptype1      122376 non-null float64\n",
      "weaptype2      8547 non-null float64\n",
      "weaptype3      1127 non-null float64\n",
      "weaptype4      63 non-null float64\n",
      "nkill          122376 non-null float64\n",
      "nwound         122376 non-null float64\n",
      "property       122376 non-null float64\n",
      "propextent     37549 non-null float64\n",
      "fatal          122376 non-null bool\n",
      "dtypes: bool(1), float64(14), int64(10)\n",
      "memory usage: 22.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "explanatory_vars = ['region','crit1','crit2','crit3','suicide','attacktype1','targtype1','individual','weaptype1', 'imonth']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(df[explanatory_vars].values, df['success'], random_state = 42, test_size = 0.2)\n",
    "\n",
    "Xtraining, Xholdout, ytraining, yholdout = train_test_split(Xtrain, ytrain, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(type(Xtrain), type(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression to predict success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899305413687\n"
     ]
    }
   ],
   "source": [
    "Log_clf = LogisticRegression()\n",
    "\n",
    "#ytrain = ytrain.reset_index()\n",
    "\n",
    "print(cv_score(Log_clf, Xtrain, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1989\n",
      "          1       0.90      1.00      0.95     17591\n",
      "\n",
      "avg / total       0.81      0.90      0.85     19580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshy\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Log_clf.fit(Xtraining, ytraining)\n",
    "print(confusion_matrix(yholdout, Log_clf.predict(Xholdout), labels = [1,0]))\n",
    "print(classification_report(yholdout, Log_clf.predict(Xholdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19580 19580\n"
     ]
    }
   ],
   "source": [
    "print(len(Log_clf.predict(Xholdout)), np.sum(Log_clf.predict(Xholdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifies every attack as a success. The accuracy score, and unweighted F1 score, are technically high but this doesn't seem very meaningful. Now I'll try regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "0.1\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "1\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "10\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "100\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "\n",
    "for c in Cs:\n",
    "    reg_clf = LogisticRegression(C = c)\n",
    "    reg_clf.fit(Xtraining, ytraining)\n",
    "    print(c)\n",
    "    print(confusion_matrix(yholdout, reg_clf.predict(Xholdout), labels = [1,0]))\n",
    "\n",
    "#for every regularization parameter, still predicts everything as success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier still classifies everything as a success. Not sure how to tune logistic regression to try to predict negatives.\n",
    "\n",
    "Next, random forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17427   164]\n",
      " [ 1722   267]]\n",
      "0.948666303756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "Rf_clf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "Rf_clf.fit(Xtraining, ytraining)\n",
    "\n",
    "print(confusion_matrix(yholdout, Rf_clf.predict(Xholdout),labels=[1,0]))\n",
    "\n",
    "print(f1_score(yholdout, Rf_clf.predict(Xholdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.13      0.22      1989\n",
      "          1       0.91      0.99      0.95     17591\n",
      "\n",
      "avg / total       0.88      0.90      0.87     19580\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yholdout, Rf_clf.predict(Xholdout)))\n",
    "print(type(classification_report(yholdout, Rf_clf.predict(Xholdout))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest predicts some failures, but the recall is still very low (0.13) for unsuccessful attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'min_impurity_decrease': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "param_grid = {'max_depth':[3,5,10,20,100], 'min_impurity_decrease':[1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2]}\n",
    "scorer = make_scorer(f1_score)\n",
    "Rf_clf = RandomForestClassifier()\n",
    "Rf_clf_cv = GridSearchCV(Rf_clf, param_grid, cv = 5, scoring = scorer)\n",
    "Rf_clf_cv.fit(Xtrain, ytrain)\n",
    "\n",
    "print(Rf_clf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.12      0.20      9858\n",
      "          1       0.91      0.99      0.95     88042\n",
      "\n",
      "avg / total       0.89      0.91      0.87     97900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Rf_clf_tuned = RandomForestClassifier(max_depth = 10, min_impurity_decrease = 1e-6)\n",
    "\n",
    "Rf_clf_tuned.fit(Xtrain,ytrain)\n",
    "print(classification_report(ytrain, Rf_clf_tuned.predict(Xtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning does not make much of a difference.\n",
    "\n",
    "Moving to fatality rate, which is closer to 50/50 which makes interpreting metrics easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.453046348957\n"
     ]
    }
   ],
   "source": [
    "print(df['fatal'].sum()/df['fatal'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738764044944\n"
     ]
    }
   ],
   "source": [
    "Xtrain_f, Xtest_f, ytrain_f, ytest_f = train_test_split(df[explanatory_vars].values, df['fatal'], random_state = 42, test_size = 0.2)\n",
    "\n",
    "Log_clf_f = LogisticRegression()\n",
    "\n",
    "print(cv_score(Log_clf_f, Xtrain_f, ytrain_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average accuracy score of 0.7387 after cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.74      0.82      0.77     53654\n",
      "       True       0.74      0.64      0.69     44246\n",
      "\n",
      "avg / total       0.74      0.74      0.74     97900\n",
      "\n",
      "0.739039836568\n"
     ]
    }
   ],
   "source": [
    "Log_clf_f = LogisticRegression()\n",
    "Log_clf_f.fit(Xtrain_f, ytrain_f)\n",
    "print(classification_report(ytrain_f, Log_clf_f.predict(Xtrain_f)))\n",
    "print(accuracy_score(ytrain_f, Log_clf_f.predict(Xtrain_f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score on training data (0.739) is approximately the same as average cross-validation score, so there is no overfitting even without regularization. F1-score is somewhat better for non-fatal attacks (0.77 vs 0.69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760847803882\n"
     ]
    }
   ],
   "source": [
    "Rf_clf_f = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "print(cv_score(Rf_clf_f, Xtrain_f, ytrain_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.81      0.81      0.81     53654\n",
      "       True       0.77      0.78      0.77     44246\n",
      "\n",
      "avg / total       0.79      0.79      0.79     97900\n",
      "\n",
      "0.792042900919\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrain_f, Rf_clf_f.predict(Xtrain_f)))\n",
    "print(accuracy_score(ytrain_f, Rf_clf_f.predict(Xtrain_f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average cross-validated score (0.76) is only slightly worse than the accuracy on training data (0.792), so there is only a little overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 100, 'min_impurity_decrease': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':[10,20,100,500,1000], 'min_impurity_decrease':[1e-9, 1e-8, 1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2]}\n",
    "scorer = make_scorer(f1_score)\n",
    "Rf_clf_f = RandomForestClassifier()\n",
    "Rf_clf_fcv = GridSearchCV(Rf_clf_f, param_grid, cv = 5, scoring = scorer)\n",
    "Rf_clf_fcv.fit(Xtrain_f, ytrain_f)\n",
    "\n",
    "print(Rf_clf_fcv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best params: max_depth 100, min_impurity_decrease 1e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766894790603\n"
     ]
    }
   ],
   "source": [
    "Rf_clf_f_tuned = RandomForestClassifier(max_depth = 100, min_impurity_decrease = 1e-06)\n",
    "\n",
    "Rf_clf_f_tuned.fit(Xtrain_f,ytrain_f)\n",
    "\n",
    "print(cv_score(Rf_clf_f_tuned, Xtrain_f, ytrain_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score after tuning is 0.767, which is a very small improvement over the untuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763258426966\n"
     ]
    }
   ],
   "source": [
    "Sv_clf_f = SVC()\n",
    "\n",
    "Sv_clf_f.fit(Xtrain_f, ytrain_f)\n",
    "\n",
    "print(accuracy_score(ytrain_f, Sv_clf_f.predict(Xtrain_f)))\n",
    "#accuracy is 0.76326. This takes a long time (>10 minutes) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "Sv_clf_f = SVC()\n",
    "\n",
    "#this takes forever...\n",
    "print(cv_score(Sv_clf_f, Xtrain_f, ytrain_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "potential variables to add: country, target subtype, number of perpetrators, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need preprocessing: binarize variables for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
