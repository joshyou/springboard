{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 122376 entries, 0 to 122375\n",
      "Data columns (total 24 columns):\n",
      "Unnamed: 0     122376 non-null int64\n",
      "iyear          122376 non-null int64\n",
      "imonth         122376 non-null int64\n",
      "region         122376 non-null int64\n",
      "crit1          122376 non-null int64\n",
      "crit2          122376 non-null int64\n",
      "crit3          122376 non-null int64\n",
      "success        122376 non-null int64\n",
      "suicide        122376 non-null int64\n",
      "attacktype1    122376 non-null float64\n",
      "attacktype2    3533 non-null float64\n",
      "attacktype3    214 non-null float64\n",
      "targtype1      122376 non-null float64\n",
      "targtype2      6685 non-null float64\n",
      "targtype3      703 non-null float64\n",
      "individual     122376 non-null int64\n",
      "weaptype1      122376 non-null float64\n",
      "weaptype2      8547 non-null float64\n",
      "weaptype3      1127 non-null float64\n",
      "weaptype4      63 non-null float64\n",
      "nkill          122376 non-null float64\n",
      "nwound         122376 non-null float64\n",
      "property       122376 non-null float64\n",
      "propextent     37549 non-null float64\n",
      "dtypes: float64(14), int64(10)\n",
      "memory usage: 22.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "explanatory_vars = ['region','crit1','crit2','crit3','suicide','attacktype1','targtype1','individual','weaptype1']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(df[explanatory_vars].values, df['success'], random_state = 42, test_size = 0.2)\n",
    "\n",
    "Xtraining, Xholdout, ytraining, yholdout = train_test_split(Xtrain, ytrain, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(type(Xtrain), type(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold, random_state = 42).split(x): # split data into train/test groups, 5 times\n",
    "        #print(\"hello\")\n",
    "        #print(x[train])\n",
    "        #print(y[train])\n",
    "        clf.fit(x[train], y.iloc[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y.iloc[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899305413687\n"
     ]
    }
   ],
   "source": [
    "Log_clf = LogisticRegression()\n",
    "\n",
    "#ytrain = ytrain.reset_index()\n",
    "\n",
    "print(cv_score(Log_clf, Xtrain, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1989\n",
      "          1       0.90      1.00      0.95     17591\n",
      "\n",
      "avg / total       0.81      0.90      0.85     19580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshy\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Log_clf.fit(Xtraining, ytraining)\n",
    "print(confusion_matrix(yholdout, Log_clf.predict(Xholdout), labels = [1,0]))\n",
    "print(classification_report(yholdout, Log_clf.predict(Xholdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19580 19580\n"
     ]
    }
   ],
   "source": [
    "print(len(Log_clf.predict(Xholdout)), np.sum(Log_clf.predict(Xholdout)))\n",
    "#the classifier predicts every attack as a success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "0.1\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "1\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "10\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n",
      "100\n",
      "[[17591     0]\n",
      " [ 1989     0]]\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "\n",
    "for c in Cs:\n",
    "    reg_clf = LogisticRegression(C = c)\n",
    "    reg_clf.fit(Xtraining, ytraining)\n",
    "    print(c)\n",
    "    print(confusion_matrix(yholdout, reg_clf.predict(Xholdout), labels = [1,0]))\n",
    "\n",
    "#for every regularization parameter, still predicts everything as success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17417   174]\n",
      " [ 1726   263]]\n",
      "0.948276800784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "Rf_clf = RandomForestClassifier()\n",
    "\n",
    "Rf_clf.fit(Xtraining, ytraining)\n",
    "\n",
    "print(confusion_matrix(yholdout, Rf_clf.predict(Xholdout),labels=[1,0]))\n",
    "#hey this predicts some failures!\n",
    "\n",
    "print(f1_score(yholdout, Rf_clf.predict(Xholdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.13      0.22      1989\n",
      "          1       0.91      0.99      0.95     17591\n",
      "\n",
      "avg / total       0.88      0.90      0.87     19580\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yholdout, Rf_clf.predict(Xholdout)))\n",
    "print(type(classification_report(yholdout, Rf_clf.predict(Xholdout))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'min_impurity_decrease': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "#i'm guessing tuning to prevent overfitting the model will mean the model will predict even more successes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "param_grid = {'max_depth':[3,5,10,20,100], 'min_impurity_decrease':[1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2]}\n",
    "scorer = make_scorer(f1_score)\n",
    "Rf_clf = RandomForestClassifier()\n",
    "Rf_clf_cv = GridSearchCV(Rf_clf, param_grid, cv = 5, scoring = scorer)\n",
    "Rf_clf_cv.fit(Xtrain, ytrain)\n",
    "\n",
    "print(Rf_clf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.12      0.20      9858\n",
      "          1       0.91      0.99      0.95     88042\n",
      "\n",
      "avg / total       0.89      0.91      0.87     97900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Rf_clf_tuned = RandomForestClassifier(max_depth = 10, min_impurity_decrease = 1e-6)\n",
    "\n",
    "Rf_clf_tuned.fit(Xtrain,ytrain)\n",
    "print(classification_report(ytrain, Rf_clf_tuned.predict(Xtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
